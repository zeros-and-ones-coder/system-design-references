<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Apache Kafka - System Design Guide</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <h1 class="nav-logo">System Design Guide</h1>
            <button class="menu-toggle" aria-label="Toggle menu">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../basics/index.html">Basics</a></li>
                <li><a href="../kafka/index.html" class="active">Kafka</a></li>
                <li><a href="../microservices/index.html">Microservices</a></li>
                <li><a href="../oauth/index.html">OAuth & OIDC</a></li>
                <li><a href="../data-security/index.html">Data Security</a></li>
            </ul>
        </div>
    </nav>

    <main class="container">
        <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <span>/</span>
            <span>Kafka</span>
        </div>

        <header class="hero">
            <h1>Apache Kafka in Production</h1>
            <p class="subtitle">Real-world patterns from running event streaming at scale</p>
        </header>

        <section class="content-section">
            <h2>Introduction</h2>
            <p>
                I've been running Kafka clusters in production for years now, processing anywhere from millions to billions of events per day. Every time I think I've seen it all, Kafka finds a new way to humble me. This guide is what I wish someone had shown me before my first production deployment—the real operational challenges, not just the happy path examples from tutorials.
            </p>

            <p>
                Here's the thing: Kafka is amazing at what it does. It's fast, durable, and scalable. But it's also complex, and the defaults are optimized for "getting started" not "running in production at 3 AM when things break." Let me show you what actually matters when you're responsible for keeping the data flowing.
            </p>

            <div class="info-box important">
                <strong>Architect Perspective:</strong>
                I've designed and operated Kafka deployments ranging from small startups (3 brokers) to enterprise scale (100+ brokers, petabytes of data). The patterns are remarkably similar. The failures are too. Learn from my mistakes so you don't have to make them yourself.
            </div>
        </section>

        <section class="content-section">
            <h2>High-Level Architecture</h2>

            <h3>The Mental Model</h3>
            <p>
                Before diving into the weeds, you need to understand how Kafka actually works. Here's the architecture from a real e-commerce system we built:
            </p>

            <pre><code>┌─────────────────────────────────────────────────────────────────┐
│                         KAFKA CLUSTER                           │
│                                                                 │
│  ┌──────────┐      ┌──────────┐      ┌──────────┐             │
│  │ Broker 1 │      │ Broker 2 │      │ Broker 3 │             │
│  │          │      │          │      │          │             │
│  │ orders-0 │─────▶│ orders-0 │─────▶│ orders-0 │  Replication│
│  │ (Leader) │      │(Follower)│      │(Follower)│             │
│  │          │      │          │      │          │             │
│  │ orders-1 │      │ orders-1 │      │ orders-1 │             │
│  │(Follower)│◀─────│ (Leader) │─────▶│(Follower)│             │
│  │          │      │          │      │          │             │
│  │ orders-2 │      │ orders-2 │      │ orders-2 │             │
│  │(Follower)│◀─────│(Follower)│◀─────│ (Leader) │             │
│  └──────────┘      └──────────┘      └──────────┘             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
       ▲                                            │
       │ Producers write                            │ Consumers read
       │ (acks=all)                                 │ (poll every 500ms)
       │                                            ▼
┌──────────────┐                            ┌─────────────────┐
│  Order       │                            │  Consumer Group │
│  Service     │                            │  "processors"   │
│              │                            │                 │
│ Publishes:   │                            │  Consumer 1     │
│ - OrderPlaced│                            │  → orders-0     │
│ - OrderPaid  │                            │                 │
│ - OrderShipped│                           │  Consumer 2     │
└──────────────┘                            │  → orders-1     │
                                            │                 │
                                            │  Consumer 3     │
                                            │  → orders-2     │
                                            └─────────────────┘

Key Concepts:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Topic: "orders" (logical grouping of events)
Partitions: 3 (orders-0, orders-1, orders-2)
Replication Factor: 3 (every partition on 3 brokers)
Consumer Group: Load balancing across consumers
Leader: Handles reads/writes for that partition
Followers: Replicate data for fault tolerance</code></pre>

            <div class="info-box note">
                <strong>Real Talk:</strong>
                This diagram represents a simple setup. In production, we run 50+ brokers with 100+ partitions per topic. But the principles are the same: producers write to leaders, replicas sync data, consumers read in parallel. Get this mental model right and everything else makes sense.
            </div>
        </section>

        <section class="content-section">
            <h2>Core Concepts from the Trenches</h2>

            <button class="collapsible">Partitions: The Unit of Parallelism</button>
            <div class="collapsible-content">
                <h3>How Many Partitions Do You Actually Need?</h3>
                <p>
                    This is the first question everyone gets wrong. Including me, multiple times. Here's the formula I actually use now:
                </p>

                <pre><code>// Start with your throughput requirements
target_throughput = 500 MB/s

// Measure (don't guess!) producer throughput per partition
// In our setup: ~10 MB/s per partition with compression
producer_limit = 10 MB/s

// Measure consumer throughput (often the bottleneck)
// Our processor: ~20 MB/s per consumer (includes DB writes)
consumer_limit = 20 MB/s

// Calculate minimum partitions needed
min_partitions = max(
    target_throughput / producer_limit,  // 500/10 = 50
    target_throughput / consumer_limit   // 500/20 = 25
)

// Add headroom for:
// - Traffic spikes (2x peak traffic)
// - Consumer rebalancing (some consumers temporarily idle)
// - Broker failures (need to redistribute load)
recommended_partitions = min_partitions * 2 = 100

// BUT: Check your broker limit
// Most brokers handle 2000-4000 partitions max
// With RF=3, 100 partitions = 300 partition-replicas
// On 3 brokers = 100 per broker ✓ Good</code></pre>

                <p>
                    <strong>Real scenario from last year:</strong> We started with 20 partitions for our analytics topic. Seemed fine at first. Six months later, Black Friday traffic hit and we needed 100 partitions. Here's the problem: <em>you can add partitions, but you can't easily reduce them</em>. And adding partitions changes the hash distribution, so existing keys end up in different partitions.
                </p>

                <div class="info-box warning">
                    <strong>Lesson Learned:</strong>
                    Over-partitioning from the start. It's easier to have unused capacity than to scramble during a traffic spike. We now default to 50-100 partitions for any production topic, even if we only need 10 today. The overhead is minimal compared to the pain of repartitioning later.
                </div>

                <h3>The Hot Partition Problem</h3>
                <p>
                    Picture this: You have 100 partitions, 100 consumers, everything's balanced. Then one consumer starts falling behind. Its lag goes from 0 to 10 million while the others are at zero. What happened?
                </p>

                <p>
                    <strong>Hot partitions.</strong> Your partition key has skewed distribution. Maybe you're partitioning by tenant ID and one customer is 80% of your traffic. All their events land on partition 42. That consumer can't keep up while the other 99 are bored.
                </p>

                <pre><code>// Bad: Skewed partitioning
producer.send(new ProducerRecord<>(
    "user-events",
    tenantId,      // tenant "acme-corp" = 80% of traffic
    event
));

// Result: partition for "acme-corp" gets hammered
// One consumer has 50M lag, others have 0

// Better: Salt the hot keys
String key = tenantId;
if (isHotTenant(tenantId)) {
    // Spread across 10 partitions using user ID
    int salt = Math.abs(userId.hashCode()) % 10;
    key = tenantId + "-" + salt;
}

producer.send(new ProducerRecord<>("user-events", key, event));

// Now "acme-corp" traffic spreads across 10 partitions
// Each handles 8% instead of 80%</code></pre>

                <p>
                    We detected this in production when our monitoring showed one consumer using 16GB RAM while others used 2GB. Took us hours to figure out it was a hot partition. Now we monitor per-partition lag, not just total lag.
                </p>
            </div>

            <button class="collapsible">Replication Factor: Durability vs Performance</button>
            <div class="collapsible-content">
                <h3>The Trade-off Nobody Tells You About</h3>
                <p>
                    Replication Factor (RF) is how many copies of your data Kafka keeps. RF=3 means every message is stored on 3 brokers. Seems simple. But here's what actually happens:
                </p>

                <pre><code>// Producer writes to leader
1. Producer sends message to Broker 1 (leader for partition 0)
2. Broker 1 writes to local disk
3. Broker 1 replicates to Broker 2 (follower)
4. Broker 1 replicates to Broker 3 (follower)
5. Broker 1 ACKs producer

// With acks=all (what you should use for critical data)
// Producer waits for ALL replicas to confirm
// Latency: max(network_to_broker2, network_to_broker3)

// Our measurements:
RF=1: ~2ms average latency   (no replication wait)
RF=3: ~8ms average latency   (wait for 2 replicas)

// But RF=1 means: broker dies = data loss
// Not acceptable for orders, payments, user data</code></pre>

                <p>
                    <strong>What we do in practice:</strong>
                </p>

                <table>
                    <thead>
                        <tr>
                            <th>Topic Type</th>
                            <th>RF</th>
                            <th>min.insync.replicas</th>
                            <th>Rationale</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Critical (orders, payments)</td>
                            <td>3</td>
                            <td>2</td>
                            <td>Can lose 1 broker, no data loss</td>
                        </tr>
                        <tr>
                            <td>Important (user events)</td>
                            <td>3</td>
                            <td>2</td>
                            <td>Same, most prod data</td>
                        </tr>
                        <tr>
                            <td>Logs, metrics</td>
                            <td>2</td>
                            <td>1</td>
                            <td>OK to lose some, save storage</td>
                        </tr>
                        <tr>
                            <td>Temporary/Cache</td>
                            <td>1</td>
                            <td>1</td>
                            <td>Ephemeral, recomputable</td>
                        </tr>
                    </tbody>
                </table>

                <div class="info-box important">
                    <strong>Critical Setting:</strong>
                    <code>min.insync.replicas=2</code> means "at least 2 replicas must ACK before write succeeds." With RF=3, you can lose 1 broker and still accept writes. With RF=3 and min.insync.replicas=1, you can lose 2 brokers but that's risky. We learned this during a rolling restart that went wrong—2 brokers down at once, writes started failing because we only had 1 replica in sync.
                </div>
            </div>

            <button class="collapsible">Consumer Groups: Parallelism and Rebalancing Hell</button>
            <div class="collapsible-content">
                <h3>The Magic and Misery of Consumer Groups</h3>
                <p>
                    Consumer groups are brilliant: multiple consumers work together to process a topic in parallel. Kafka automatically distributes partitions across consumers. When a consumer dies, Kafka rebalances. Sounds great, right?
                </p>

                <p>
                    It is great. Until you experience a rebalance storm at 2 AM and all your consumers stop processing for 30 seconds.
                </p>

                <h4>How Rebalancing Works (Simplified)</h4>
                <pre><code>// You have: 100 partitions, 10 consumers in group "processors"
// Kafka assigns: 10 partitions per consumer (balanced)

Consumer 1 → partitions 0-9
Consumer 2 → partitions 10-19
...
Consumer 10 → partitions 90-99

// Consumer 5 crashes
// Kafka detects: heartbeat timeout (default 10 seconds)
// Rebalance triggered:

1. ALL consumers stop processing
2. Coordinator reassigns partitions
   Consumer 1 → partitions 0-9, 40-49  (gets extra 10)
   Consumer 2 → partitions 10-19
   ...
   Consumer 10 → partitions 90-99
3. Consumers commit offsets, seek to new positions
4. Processing resumes

// Total pause time: 15-45 seconds typically
// For 10,000 msgs/sec topic: 150,000-450,000 messages delayed</code></pre>

                <p>
                    <strong>The Rebalance Storm:</strong> We deployed a new version using Kubernetes rolling update. It restarted consumers one by one, every 30 seconds. Each restart triggered a rebalance. For 10 minutes, our consumers were rebalancing more than processing. Lag spiked to 5 million.
                </p>

                <h4>How We Fixed It</h4>
                <pre><code>// 1. Increase session timeout (how long before Kafka marks consumer dead)
session.timeout.ms = 30000  // 30 seconds (was 10s)

// 2. Increase heartbeat interval
heartbeat.interval.ms = 10000  // 10 seconds (was 3s)

// 3. Increase max poll interval (time between poll() calls)
max.poll.interval.ms = 300000  // 5 minutes (was 5 minutes, but good to know)

// 4. Use static membership (Kafka 2.4+)
// Consumers get fixed IDs, so restart doesn't trigger rebalance
group.instance.id = "consumer-1"  // Set per instance

// Result: Kubernetes rolling restart no longer causes rebalance
// Consumer restarts within 30 seconds = no rebalance triggered</code></pre>

                <div class="info-box tip">
                    <strong>Pro Tip:</strong>
                    Monitor rebalances. We track <code>rebalance_count</code> and <code>rebalance_duration_ms</code>. If you're rebalancing more than once per hour, something's wrong. Common causes: consumers timing out due to slow processing, network issues, or overly aggressive timeout settings.
                </div>

                <h4>The N+1 Partition Problem</h4>
                <p>
                    You have 100 partitions and 100 consumers. Perfect 1:1 mapping. Then someone adds one more partition. Now you have 101 partitions and 100 consumers. Kafka can't split a partition across consumers, so one consumer gets 2 partitions. That consumer now processes 2x the data and falls behind.
                </p>

                <p>
                    <strong>Solution:</strong> Plan partitions in advance. Don't add them incrementally. And keep consumers >= partitions. We run 120 consumers for 100 partitions so we have buffer for restarts and failures.
                </p>
            </div>
        </section>

        <section class="content-section">
            <h2>Production War Stories</h2>

            <button class="collapsible">The Great Poison Pill Incident</button>
            <div class="collapsible-content">
                <h3>When One Bad Message Stops Everything</h3>
                <p>
                    <strong>What happened:</strong> 3 AM on a Tuesday. Pager goes off. Consumer lag spiking. I log in, check the consumer logs. Exception: <code>JsonParseException: Unexpected character '&lt;' at position 0</code>. One consumer stuck processing the same message over and over. The other consumers? All fine, happily processing millions of messages.
                </p>

                <p>
                    The problem: A producer sent an HTML error page instead of JSON. That message landed on partition 17. The consumer for partition 17 couldn't deserialize it, threw an exception, and... retried the same message. Forever.
                </p>

                <h4>The Naive Approach (Don't Do This)</h4>
                <pre><code>// This will loop forever on poison pill
while (true) {
    ConsumerRecords records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord record : records) {
        processRecord(record);  // Throws exception on bad data
        consumer.commitSync();   // Never reached!
    }
}</code></pre>

                <h4>What We Do Now</h4>
                <pre><code>while (true) {
    ConsumerRecords records = consumer.poll(Duration.ofMillis(100));

    for (ConsumerRecord record : records) {
        try {
            processRecord(record);
            consumer.commitSync();  // Success

        } catch (DeserializationException e) {
            // Can't parse message - it's poison
            log.error("Poison pill detected, sending to DLQ", e);

            // Send to Dead Letter Queue for manual inspection
            dlqProducer.send(new ProducerRecord<>(
                "orders-dlq",
                record.key(),
                record.value()
            ));

            // Skip this message and move on
            consumer.commitSync();

            // Alert the team
            alerting.sendAlert("Poison pill in orders topic", WARN);

        } catch (RetriableException e) {
            // Temporary failure (DB down, API timeout)
            // Don't commit, retry this message
            log.warn("Retriable error, will retry", e);
            break;  // Exit loop, don't commit, will retry on next poll

        } catch (Exception e) {
            // Unknown error - send to DLQ to be safe
            log.error("Unknown error, sending to DLQ", e);
            dlqProducer.send(new ProducerRecord<>("orders-dlq", record));
            consumer.commitSync();
        }
    }
}</code></pre>

                <p>
                    <strong>The lesson:</strong> Always have a Dead Letter Queue (DLQ). Always handle deserialization errors separately from business logic errors. And always commit the offset after sending to DLQ, or you'll loop forever.
                </p>

                <div class="info-box warning">
                    <strong>Don't Forget:</strong>
                    Actually monitor your DLQ! We've found critical bugs days later because we sent messages to DLQ but nobody was watching it. Set up alerts when DLQ receives messages.
                </div>
            </div>

            <button class="collapsible">The API Outage That Killed Kafka</button>
            <div class="collapsible-content">
                <h3>When Your Downstream Dependency Goes Down</h3>
                <p>
                    <strong>The scenario:</strong> Our consumer reads events from Kafka and calls an external API for each event. Processing 10,000 events/second, everything's great. Then the API goes down for 30 minutes.
                </p>

                <p>
                    What we expected: Consumer would retry, eventually the API comes back, we catch up.
                </p>

                <p>
                    What actually happened: Our consumer sent 18 million failed events to the DLQ (10k/sec × 30 min × 60 sec). The DLQ topic grew to 200GB. When the API came back online, we had to process both the regular queue AND 18 million DLQ messages. It took 20 hours to recover.
                </p>

                <h4>The Problem with Naive DLQ</h4>
                <pre><code>// This seems reasonable...
try {
    apiClient.call(record.value());
    consumer.commitSync();
} catch (ApiException e) {
    // API call failed, send to DLQ
    dlqProducer.send(record);  // 18 million times!
    consumer.commitSync();
}

// But API outage is NOT the same as a poison pill
// Poison pill: one bad message
// API outage: ALL messages fail temporarily</code></pre>

                <h4>The Circuit Breaker Solution</h4>
                <pre><code>// Detect when API is down and STOP consuming
CircuitBreaker breaker = CircuitBreaker.builder()
    .failureRateThreshold(50)  // Open if 50% fail
    .slidingWindowSize(100)
    .waitDurationInOpenState(Duration.ofMinutes(2))
    .build();

// Listen for circuit state changes
breaker.getEventPublisher().onStateTransition(event -> {
    if (event.getStateTransition().getToState() == State.OPEN) {
        // API is down - pause Kafka consumption
        consumer.pause(consumer.assignment());
        log.warn("Circuit OPEN, paused Kafka. Lag will grow.");
        alert("Consumer paused due to API failure");
    } else if (event.getStateTransition().getToState() == State.CLOSED) {
        // API recovered - resume consumption
        consumer.resume(consumer.assignment());
        log.info("Circuit CLOSED, resumed Kafka consumption");
    }
});

// Process with circuit breaker
while (true) {
    ConsumerRecords records = consumer.poll(Duration.ofMillis(100));

    for (ConsumerRecord record : records) {
        try {
            // Wrap API call in circuit breaker
            breaker.executeSupplier(() -> apiClient.call(record));
            consumer.commitSync();

        } catch (CallNotPermittedException e) {
            // Circuit is open, consumer should be paused
            // Just wait, don't send to DLQ

        } catch (ApiException e) {
            // Circuit is closed but this particular call failed
            // Could be poison pill - send to DLQ
            dlqProducer.send(record);
            consumer.commitSync();
        }
    }
}</code></pre>

                <p>
                    <strong>The result:</strong> When API goes down, circuit opens after ~100 failed requests (few seconds). Consumer pauses. Lag grows in Kafka (which is designed for this). API comes back, circuit closes, consumer resumes, lag clears in minutes instead of hours.
                </p>

                <table>
                    <thead>
                        <tr>
                            <th>Approach</th>
                            <th>DLQ Messages</th>
                            <th>Recovery Time</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Naive DLQ</td>
                            <td>18 million</td>
                            <td>20 hours</td>
                        </tr>
                        <tr>
                            <td>Circuit Breaker</td>
                            <td>~100</td>
                            <td>15 minutes</td>
                        </tr>
                    </tbody>
                </table>

                <div class="info-box important">
                    <strong>The Big Lesson:</strong>
                    Kafka lag is OK. Kafka is designed to store messages. DLQ overflow is NOT OK. Your DLQ should only have genuine poison pills—dozens to hundreds of messages, not millions. Use circuit breakers to detect systemic failures and pause consumption.
                </div>
            </div>

            <button class="collapsible">The Disk Space Death Spiral</button>
            <div class="collapsible-content">
                <h3>When Retention Policies Bite You</h3>
                <p>
                    <strong>4 AM alert:</strong> "Broker 3 disk 95% full." I check the broker. Partition <code>analytics-logs-47</code> is 800GB. The next biggest partition is 50GB. What happened?
                </p>

                <p>
                    We set retention to 7 days. But we also set <code>retention.bytes=-1</code> (unlimited). During a traffic spike, we wrote 2TB to this topic in 3 days. Retention by time hadn't kicked in yet. The partition kept growing. Broker ran out of disk space. Writes started failing.
                </p>

                <h4>Retention Settings That Actually Work</h4>
                <pre><code>// Set BOTH time and size limits
retention.ms = 604800000          // 7 days
retention.bytes = 107374182400     // 100GB per partition

// Whichever limit hits first, data gets deleted
// Protects against traffic spikes

// For critical topics (orders, payments)
retention.ms = 2592000000          // 30 days
retention.bytes = 536870912000     // 500GB per partition

// For high-volume logs
retention.ms = 86400000            // 1 day
retention.bytes = 53687091200      // 50GB per partition</code></pre>

                <p>
                    <strong>The cleanup process:</strong> Kafka deletes old segments when <em>the entire segment</em> is past retention. Segments are created when <code>segment.bytes</code> is reached (default 1GB). This means:
                </p>

                <ul>
                    <li>If you write 1GB/hour, you get hourly segments, cleaned up hourly</li>
                    <li>If you write 1GB/day, you get daily segments, cleaned up daily</li>
                    <li>If you write slowly, segments might not roll over, retention doesn't work!</li>
                </ul>

                <pre><code>// Force segment rollover even if size not reached
segment.ms = 86400000  // 1 day
// This ensures retention can happen at least daily

// For high-throughput topics
segment.bytes = 1073741824  // 1GB (default)
segment.ms = 3600000        // 1 hour
// Smaller, more frequent segments = more predictable cleanup</code></pre>

                <div class="info-box warning">
                    <strong>Death Spiral:</strong>
                    Disk full → Broker can't write → Leader election fails → Cluster unstable → More disk fills up → Cascade failure. We had to delete data manually to recover. Monitor disk usage aggressively. Alert at 70%, page at 85%.
                </div>
            </div>
        </section>

        <section class="content-section">
            <h2>Production Best Practices</h2>

            <h3>Producer Configuration</h3>
            <p>
                After burning myself too many times, here's what I use for production:
            </p>

            <pre><code>// For critical data (orders, payments, user data)
Properties props = new Properties();
props.put("acks", "all");                    // Wait for all replicas
props.put("retries", Integer.MAX_VALUE);     // Retry forever (with backoff)
props.put("max.in.flight.requests.per.connection", 5);  // Kafka 1.0+ safe
props.put("enable.idempotence", true);       // Exactly-once semantics
props.put("compression.type", "lz4");        // Fast, good ratio
props.put("batch.size", 32768);              // 32KB batches
props.put("linger.ms", 10);                  // Wait up to 10ms for batch

// For logs, metrics (lossy is OK)
props.put("acks", "1");                      // Just leader ACK
props.put("retries", 3);                     // Give up after 3 tries
props.put("compression.type", "snappy");     // Even faster
props.put("linger.ms", 100);                 // Batch more aggressively

// Common to both
props.put("buffer.memory", 67108864);        // 64MB buffer
props.put("delivery.timeout.ms", 120000);    // 2 min total timeout</code></pre>

            <div class="info-box tip">
                <strong>Idempotence is Magic:</strong>
                With <code>enable.idempotence=true</code>, Kafka deduplicates retries. Producer crashes and retries? No duplicate messages. We enable this everywhere now. The overhead is negligible.
            </div>

            <h3>Consumer Configuration</h3>
            <pre><code>// Production consumer settings
props.put("enable.auto.commit", false);      // Manual commits only
props.put("auto.offset.reset", "earliest");  // Don't skip data on new consumer
props.put("max.poll.records", 500);          // Process in batches
props.put("session.timeout.ms", 30000);      // 30s before marked dead
props.put("heartbeat.interval.ms", 10000);   // Heartbeat every 10s
props.put("max.poll.interval.ms", 300000);   // 5 min processing time OK

// For fast recovery from rebalancing
props.put("group.instance.id", "consumer-1"); // Static membership
props.put("partition.assignment.strategy",
    "org.apache.kafka.clients.consumer.CooperativeStickyAssignor");

// Fetch settings for throughput
props.put("fetch.min.bytes", 1024);          // Wait for 1KB
props.put("fetch.max.wait.ms", 500);         // Or 500ms</code></pre>

            <h3>Topic Configuration</h3>
            <pre><code>// Create topic with proper settings
kafka-topics.sh --create --topic orders \
  --bootstrap-server localhost:9092 \
  --partitions 100 \
  --replication-factor 3 \
  --config min.insync.replicas=2 \
  --config retention.ms=2592000000 \     # 30 days
  --config retention.bytes=536870912000 \ # 500GB per partition
  --config segment.ms=3600000 \           # 1 hour segments
  --config compression.type=producer \     # Use producer's compression
  --config cleanup.policy=delete          # Delete old data (vs compact)</code></pre>

            <h3>Monitoring That Matters</h3>
            <p>
                You can monitor a million metrics. These are the ones that actually saved me:
            </p>

            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Alert At</th>
                        <th>Why It Matters</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Consumer Lag</td>
                        <td>> 100K messages</td>
                        <td>Consumer falling behind</td>
                    </tr>
                    <tr>
                        <td>Lag Growth Rate</td>
                        <td>Increasing for 5 min</td>
                        <td>Consumer can't keep up</td>
                    </tr>
                    <tr>
                        <td>Rebalance Frequency</td>
                        <td>> 1/hour</td>
                        <td>Consumers unstable</td>
                    </tr>
                    <tr>
                        <td>Under-Replicated Partitions</td>
                        <td>> 0</td>
                        <td>Data at risk</td>
                    </tr>
                    <tr>
                        <td>Disk Usage</td>
                        <td>> 70%</td>
                        <td>About to run out of space</td>
                    </tr>
                    <tr>
                        <td>Offline Partitions</td>
                        <td>> 0</td>
                        <td>Data unavailable!</td>
                    </tr>
                    <tr>
                        <td>DLQ Write Rate</td>
                        <td>> 10/min</td>
                        <td>Poison pills or API issues</td>
                    </tr>
                </tbody>
            </table>

            <pre><code>// Check consumer lag
kafka-consumer-groups.sh --describe \
  --group order-processor \
  --bootstrap-server localhost:9092

// Look for LAG column
// If lag is growing → consumer can't keep up
// If lag is on one partition → hot partition problem
// If lag across all partitions → systemic issue (API down, slow DB)</code></pre>
        </section>

        <section class="content-section">
            <h2>Common Mistakes (That I Made)</h2>

            <h3>1. Not Testing Consumer Failure</h3>
            <p>
                We tested happy path: produce message, consume message, process, done. We didn't test: what if consumer crashes mid-processing? What if the same message is processed twice?
            </p>
            <p>
                <strong>Result:</strong> Duplicate orders. User ordered one item, got charged twice because consumer crashed after processing but before committing offset.
            </p>
            <p>
                <strong>Fix:</strong> Make consumers idempotent. Use unique IDs. Check "did I already process this?" before processing.
            </p>

            <pre><code>void processOrder(Order order) {
    // Check if already processed
    if (db.exists("processed_orders", order.id)) {
        log.info("Duplicate message, skipping: {}", order.id);
        return;  // Already processed
    }

    // Process order
    createInvoice(order);
    chargePayment(order);

    // Mark as processed (in same transaction if possible!)
    db.insert("processed_orders", order.id);

    // Now safe to commit Kafka offset
    consumer.commitSync();
}</code></pre>

            <h3>2. Using Synchronous Commits Everywhere</h3>
            <p>
                <code>consumer.commitSync()</code> blocks until Kafka confirms. If you process 1000 messages and commit after each one, you're adding 1000 network round-trips. We were processing 10K msgs/sec but throughput capped at 500/sec because of commit latency.
            </p>

            <pre><code>// Slow
for (ConsumerRecord record : records) {
    process(record);
    consumer.commitSync();  // Block for every message
}

// Fast
for (ConsumerRecord record : records) {
    process(record);
}
consumer.commitSync();  // Block once per batch

// Even faster with async
for (ConsumerRecord record : records) {
    process(record);
}
consumer.commitAsync((offsets, exception) -> {
    if (exception != null) {
        log.error("Commit failed", exception);
    }
});</code></pre>

            <h3>3. Not Planning for Growth</h3>
            <p>
                Started with 10 partitions because "we only need 5 consumers today." Six months later, Black Friday traffic, we need 100 partitions. Adding partitions changes key distribution. Existing messages with key "order-123" were on partition 7. After repartitioning, new messages with same key go to partition 47. Broke our ordering guarantees.
            </p>
            <p>
                <strong>Lesson:</strong> Over-partition from day one. Unused partitions cost almost nothing. Repartitioning costs everything.
            </p>
        </section>

        <section class="content-section">
            <h2>When Things Go Wrong: The Runbook</h2>

            <h3>Consumer Lag Spiking</h3>
            <pre><code>// 1. Check if lag is growing
kafka-consumer-groups.sh --describe --group my-group

// 2. Is it one partition or all?
// One partition → hot partition, slow consumer, poison pill
// All partitions → systemic issue (API down, DB slow)

// 3. Check consumer logs for errors

// 4. Temporary fix: Scale up consumers
kubectl scale deployment my-consumer --replicas=20

// 5. Long-term fix: Identify bottleneck
// - Hot partition → repartition with salt
// - Slow DB → add index, optimize query
// - API down → implement circuit breaker</code></pre>

            <h3>Broker Down</h3>
            <pre><code>// Kafka is designed for this, BUT:

// 1. Check under-replicated partitions
kafka-topics.sh --describe --under-replicated-partitions

// 2. If broker coming back soon (< 5 min): do nothing, Kafka handles it
// 3. If broker lost permanently: reassign partitions

// Generate reassignment plan
kafka-reassign-partitions.sh --generate \
  --topics-to-move-json-file topics.json \
  --broker-list "0,1,2,4,5"  # Excluding dead broker 3

// Execute reassignment
kafka-reassign-partitions.sh --execute \
  --reassignment-json-file reassignment.json

// This takes time! 100GB partition = 10+ minutes to replicate</code></pre>

            <h3>Disk Full</h3>
            <pre><code>// Emergency procedure (use with extreme caution!)

// 1. Identify largest partitions
du -sh /var/lib/kafka/data/* | sort -h | tail -20

// 2. Reduce retention on largest topics
kafka-configs.sh --alter --topic huge-topic \
  --add-config retention.ms=86400000  # 1 day

// 3. Force log cleanup (wait 5-10 min for cleanup to run)

// 4. If still critical: delete old log segments MANUALLY
# Find oldest segments
ls -lt /var/lib/kafka/data/huge-topic-0/*.log
# Delete oldest (DANGER: data loss!)
rm /var/lib/kafka/data/huge-topic-0/00000000000000000000.log

// 5. Long-term: Add more disk or separate topics to different volumes</code></pre>
        </section>

        <section class="content-section">
            <h2>Key Takeaways</h2>

            <div class="info-box important">
                <strong>If I could go back and tell myself five things before my first Kafka production deployment:</strong>
            </div>

            <ol>
                <li>
                    <strong>Over-partition from the start.</strong> 50-100 partitions is fine even if you only need 10 today. You can't easily reduce partitions, and adding them changes key distribution. The overhead is minimal, the flexibility is priceless.
                </li>
                <li>
                    <strong>Always use Dead Letter Queues, but protect them with circuit breakers.</strong> DLQ should have dozens of poison pills, not millions of failed API calls. When your downstream API goes down, pause the consumer—don't flood the DLQ.
                </li>
                <li>
                    <strong>Make everything idempotent.</strong> Kafka will deliver messages at least once. Consumers crash. Networks fail. You will process duplicates. Design for it from day one, not after you've charged customers twice.
                </li>
                <li>
                    <strong>Monitor per-partition metrics, not just totals.</strong> Consumer group lag looks fine at 100K total, but partition 42 has 99K of that lag. Hot partitions are invisible in aggregate metrics. We spent hours debugging issues we would've seen immediately in partition-level dashboards.
                </li>
                <li>
                    <strong>Test failure scenarios.</strong> Producer crashes mid-send. Consumer crashes mid-processing. Broker goes down during rebalance. Network partitions. Run these scenarios in staging. The pain you feel in staging saves you from 3 AM pages in production.
                </li>
            </ol>

            <p>
                Kafka is an incredible piece of technology. It powers some of the largest systems in the world. But it's complex, and the defaults are optimized for tutorials, not production. Learn from my scars. Configure it properly. Monitor it religiously. And always, always have a runbook ready for when things go sideways at 3 AM.
            </p>
        </section>

        <section class="content-section">
            <h2>Additional Resources</h2>
            <ul>
                <li><a href="https://kafka.apache.org/documentation/" target="_blank">Apache Kafka Official Documentation</a></li>
                <li><a href="https://www.confluent.io/blog/" target="_blank">Confluent Blog - Kafka Best Practices</a></li>
                <li><a href="https://github.com/linkedin/Burrow" target="_blank">Burrow - Kafka Consumer Lag Monitoring</a></li>
                <li><a href="https://github.com/linkedin/cruise-control" target="_blank">Cruise Control - Kafka Cluster Management</a></li>
                <li><a href="https://kafka.apache.org/documentation/#producerconfigs" target="_blank">Producer Configuration Reference</a></li>
                <li><a href="https://kafka.apache.org/documentation/#consumerconfigs" target="_blank">Consumer Configuration Reference</a></li>
            </ul>
        </section>

        <section class="content-section">
            <h2>Related Topics</h2>
            <div class="topics-grid">
                <div class="topic-card">
                    <h3>Microservices Architecture</h3>
                    <p>Learn how Kafka fits into microservices communication patterns and event-driven architecture.</p>
                    <a href="../microservices/index.html" class="btn">Learn More →</a>
                </div>
                <div class="topic-card">
                    <h3>System Design Basics</h3>
                    <p>Understand the fundamentals of distributed systems, scalability, and reliability patterns.</p>
                    <a href="../basics/index.html" class="btn">Learn More →</a>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2026 System Design Reference Guide | Your Personal Learning Resource</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
